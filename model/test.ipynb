{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/wtw/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/wtw/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassifierOutput(loss=None, logits=tensor([[-1.5074,  2.1681,  1.5576, -1.2671, -1.5802,  0.3297, -1.3380, -1.2168,\n",
      "          1.7721, -0.0932,  0.1700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "idx is 1, value is fogsmog\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "1\n",
    "url = 'https://img.freepik.com/free-photo/grassy-field-with-leafless-trees-distance-cloudy-sky-background_181624-4535.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "labels = ['dew', 'fogsmog', 'frost', 'glaze', 'hail', 'lightning', 'rain', 'rainbow', 'rime', 'sandstorm', 'snow']\n",
    "processor = ViTImageProcessor.from_pretrained('DunnBC22/vit-base-patch16-224-in21k-weather-images-classification',\n",
    "                                              num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)})\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'DunnBC22/vit-base-patch16-224-in21k-weather-images-classification',\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "import torch\n",
    "\n",
    "print(outputs)\n",
    "idx = torch.argmax(outputs.logits)\n",
    "print(f\"idx is {idx}, value is {labels[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1_data_path = \"/Users/imdohun/PycharmProjects/wtw/AI/data/5-class_weather_status_image_classification\"\n",
    "source2_data_path = \"/Users/imdohun/PycharmProjects/wtw/AI/data/weather_image_recognition\"\n",
    "merged_data_path = \"/Users/imdohun/PycharmProjects/wtw/AI/data/merged_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1_folders = glob(f\"{source1_data_path}/*\")\n",
    "source2_folders = glob(f\"{source2_data_path}/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder2target_folder = {\n",
    "    # source1 (11종) \n",
    "    \"fogsmog\": \"foggy\",\n",
    "    \"lightning\": \"rainy\",\n",
    "    \"rain\": \"rainy\",\n",
    "    \"sandstorm\": \"cloudy\",\n",
    "    \"snow\": \"snowy\",\n",
    "    # source2 (5종)\n",
    "    \"cloudy\": \"cloudy\",\n",
    "    \"foggy\": \"foggy\",\n",
    "    \"rainy\": \"rainy\",\n",
    "    \"snowy\": \"snowy\",\n",
    "    \"sunny\": \"sunny\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source2folders = {\n",
    "    \"source1\": source1_folders,\n",
    "    \"source2\": source2_folders,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "for source_name, folders in source2folders.items():\n",
    "    for folder in folders:\n",
    "        folder_name = folder.split(\"/\")[-1]\n",
    "        target_folder_name = source_folder2target_folder.get(folder_name, None)\n",
    "        # 사용하지 않는 폴더의 경우 skip\n",
    "        if target_folder_name is None:\n",
    "            continue\n",
    "        \n",
    "        target_path = f\"{merged_data_path}/{target_folder_name}\"\n",
    "        \n",
    "        files = glob(f\"{folder}/*\")\n",
    "        for file in files:\n",
    "            file_name = file.split(\"/\")[-1]\n",
    "            file_name = f\"{source_name}_{file_name}\"\n",
    "            copyfile(file, f\"{target_path}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder cloudy's len is 7394\n",
      "folder sunny's len is 6274\n",
      "folder rainy's len is 2830\n",
      "folder snowy's len is 2496\n",
      "folder foggy's len is 2112\n"
     ]
    }
   ],
   "source": [
    "folders = glob(f\"{merged_data_path}/*\")\n",
    "for folder in folders:\n",
    "    folder_name = folder.split(\"/\")[-1]\n",
    "    files = glob(f\"{folder}/*\")\n",
    "    print(f\"folder {folder_name}'s len is {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source1_c0323.jpg\n",
      "source1_4659.jpg\n",
      "source1_rainy day-139.jpeg\n",
      "source1_2236953011.jpg\n",
      "source1_s2875.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "classes = [\"cloudy\", \"foggy\", \"rainy\", \"snowy\", \"sunny\"]\n",
    "\n",
    "meta_df = pd.DataFrame()\n",
    "new_parent_dir = os.path.join(merged_data_path, \"data\")\n",
    "for x in classes:\n",
    "    list_of_file_names = os.path.join(merged_data_path, x)\n",
    "    samples = os.listdir(list_of_file_names)\n",
    "    samples = random.sample(samples, 1000)\n",
    "    print(samples[0])\n",
    "    for sample in samples:\n",
    "        file_name = sample.split(\"/\")[-1]\n",
    "        copyfile(f\"{merged_data_path}/{x}/{sample}\", f\"{merged_data_path}/data/{file_name}\")\n",
    "        \n",
    "    temp_df = pd.DataFrame(samples, columns=['file_name'])\n",
    "    temp_df['label'] = str(x)\n",
    "    meta_df = pd.concat([meta_df, temp_df])\n",
    "\n",
    "# Create new directory\n",
    "\n",
    "os.makedirs(new_parent_dir, exist_ok=True)\n",
    "where_to_save_meta_file = os.path.join(new_parent_dir, \"metadata.csv\")\n",
    "meta_df.to_csv(where_to_save_meta_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4951)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta_df), len(glob(f\"{'/Users/imdohun/PycharmProjects/wtw/AI/data/merged_data/data/*'}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import PIL.Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, Image, load_metric, DatasetDict, ClassLabel\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 4951/4951 [00:00<00:00, 1310405.70files/s]\n",
      "Generating train split: 4950 examples [00:00, 33235.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lables:  ['cloudy', 'foggy', 'rainy', 'snowy', 'sunny']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 4950/4950 [00:00<00:00, 150712.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 4950\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = f\"{merged_data_path}/data\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "label_names = sorted(set(dataset[\"train\"][\"label\"]))\n",
    "print(\"lables: \", label_names)\n",
    "dataset = dataset.cast_column(\"label\", ClassLabel(names=label_names))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "train_split = dataset['train'].train_test_split(train_size=0.8)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train' : train_split['train'],\n",
    "    'test' : train_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 3960\n",
      "})\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x733 at 0x172337B80>, 'label': 3}\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x200 at 0x312EED2B0>, 'label': 4}\n",
      "Testing Dataset\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 990\n",
      "})\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x328 at 0x312EED7C0>, 'label': 3}\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x200 at 0x312EED5E0>, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset\")\n",
    "print(ds['train'])\n",
    "print(ds['train'][0])\n",
    "print(ds['train'][-1])\n",
    "\n",
    "print(\"Testing Dataset\")\n",
    "print(ds['test'])\n",
    "print(ds['test'][0])\n",
    "print(ds['test'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid_of_examples(ds, \n",
    "                          seed: int = 42, \n",
    "                          examples_per_class: int = 3, \n",
    "                          size=(350, 350)):\n",
    "    '''\n",
    "    This function displays a few pictures\n",
    "    from each class in the dataset.\n",
    "    '''\n",
    "    w, h = size\n",
    "    labels = ds['train'].features['label'].names\n",
    "    grid = PIL.Image.new(mode='RGB', size=(examples_per_class * w, len(labels) * h))\n",
    "    draw = ImageDraw.Draw(grid)\n",
    "    font = ImageFont.truetype(\"Chalkduster.ttf\", 24)\n",
    "    \n",
    "    for label_id, label in enumerate(labels):\n",
    "        # filter the dataset by a single label, shuffle it, then grab a few samples\n",
    "        ds_slice = ds['train'] \\\n",
    "                    .filter(lambda ex: ex['label'] == label_id) \\\n",
    "                    .shuffle(seed) \\\n",
    "                    .select(range(examples_per_class))\n",
    "        \n",
    "        # plot this label's examples in a row\n",
    "        for i, example in enumerate(ds_slice):\n",
    "            image = example['image']\n",
    "            idx = examples_per_class * label_id + i\n",
    "            box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
    "            grid.paste(image.resize(size), box=box)\n",
    "            draw.text(box, label, (255, 255, 255), font=font, dill=(0,0,255,1.0))\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = 'google/vit-base-patch16-224-in21k'\n",
    "MODEL_NAME=MODEL_CKPT + \"-weather_images_classification\"\n",
    "NUM_OF_EPOCHS = 1\n",
    "\n",
    "LEARNING_RATE = 2e-4\n",
    "STEPS = 100\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "# REPORTS_TO = 'tensorboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/wtw/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample_batch):\n",
    "    # take a list of PIL images and turn them into pixel values\n",
    "    inputs = feature_extractor([x.convert(\"RGB\") for x in sample_batch['image']], return_tensors=\"pt\")\n",
    "    \n",
    "    # prepare labels\n",
    "    inputs['labels'] = sample_batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    return {\n",
    "        'pixel_values' : torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels' : torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    accuracy = accuracy_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)['accuracy']\n",
    "    \n",
    "    ### ------------------- F1 scores -------------------\n",
    "    \n",
    "    f1_score_metric = evaluate.load(\"f1\")\n",
    "    weighted_f1_score = f1_score_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')[\"f1\"]\n",
    "    micro_f1_score = f1_score_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='micro')['f1']\n",
    "    macro_f1_score = f1_score_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='macro')[\"f1\"]\n",
    "    \n",
    "    ### ------------------- recall -------------------\n",
    "    \n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    weighted_recall = recall_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')[\"recall\"]\n",
    "    micro_recall = recall_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='micro')[\"recall\"]\n",
    "    macro_recall = recall_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='macro')[\"recall\"]\n",
    "    \n",
    "    ### ------------------- precision -------------------\n",
    "    \n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    weighted_precision = precision_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')[\"precision\"]\n",
    "    micro_precision = precision_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='micro')[\"precision\"]\n",
    "    macro_precision = precision_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='macro')[\"precision\"]\n",
    "    \n",
    "    return {\"accuracy\" : accuracy, \n",
    "            \"Weighted F1\" : weighted_f1_score,\n",
    "            \"Micro F1\" : micro_f1_score,\n",
    "            \"Macro F1\" : macro_f1_score,\n",
    "            \"Weighted Recall\" : weighted_recall,\n",
    "            \"Micro Recall\" : micro_recall,\n",
    "            \"Macro Recall\" : macro_recall,\n",
    "            \"Weighted Precision\" : weighted_precision,\n",
    "            \"Micro Precision\" : micro_precision,\n",
    "            \"Macro Precision\" : macro_precision\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labels = dataset['train'].features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/wtw/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"Weighted F1\",\n",
    "    logging_first_step=True,\n",
    "    # hub_private_repo=True,\n",
    "    # push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args= args,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset=prepped_ds['train'],\n",
    "    eval_dataset=prepped_ds['test'],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/248 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# !tmux new -s train\n",
    "train_results = trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wtw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
